{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import split\n",
    "from torch.optim import Adam\n",
    "from utils import LayerOutputs, UnitLength\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance2_to_centroids(h, y_true, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared distance to the centroid of each class. \n",
    "\n",
    "    Returns a tensor of shape [n_examples, 10].\n",
    "    \"\"\"\n",
    "    safe_mean = lambda x, dim: x.sum(dim) / (x.shape[dim] + epsilon)\n",
    "    # TODO: what if class is missing? determine centroids only for classes that are present, and return torch.unique(y_true)\n",
    "    class_centroids = torch.stack([safe_mean(h[y_true == i],0) for i in range(10)], dim=1) # [n_in, 10]\n",
    "    x_to_centroids = h.unsqueeze(2) - class_centroids # [n_examples, n_in, 10]\n",
    "    return x_to_centroids.pow(2).mean(1) # [n_examples, 10]\n",
    "    \n",
    "@torch.no_grad()\n",
    "def predict(model, x, y_true):\n",
    "    \"\"\"Predict by finding the class with closest centroid to each example.\"\"\"\n",
    "    d = sum(distance2_to_centroids(h, y_true) for h in LayerOutputs(model, x))\n",
    "    return d.argmin(1) # type: ignore\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_loss(h, y_true, alpha=10, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Loss function based on distance^2 to the true centroid vs a nearby centroid.\n",
    "    \n",
    "    Achieves an error rate of ~2.0%.\n",
    "    \"\"\"\n",
    "\n",
    "    # Distance from h to centroids of each class\n",
    "    d2 = distance2_to_centroids(h, y_true)\n",
    "\n",
    "    # Choose a nearby class, at random, using the inverse distance as a\n",
    "    # probability distribution\n",
    "    y_near = torch.multinomial((1 / (d2 + epsilon)), 1).squeeze(1)\n",
    "\n",
    "    # Smoothed version of triplet loss: max(0, d2_same - d2_near + margin)\n",
    "    d2_true = d2[range(d2.shape[0]), y_true] # ||anchor - positive||^2\n",
    "    d2_near = d2[range(d2.shape[0]), y_near] # ||anchor - negative||^2\n",
    "    return F.silu(alpha * (d2_true - d2_near)).mean()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# The data is pre-processed, to speed up this script\n",
    "x_tr = torch.load('./data/MNIST/baked/train_x.pt', device)\n",
    "y_tr = torch.load('./data/MNIST/baked/train_y.pt', device)\n",
    "x_te = torch.load('./data/MNIST/baked/test_x.pt', device)\n",
    "y_te = torch.load('./data/MNIST/baked/test_y.pt', device)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# ----------------\n",
    "# Must be an iterable of layers. I find it works best if each layer starts with\n",
    "# a UnitLength() sub-layer.\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(UnitLength(), nn.Linear(784, 500), nn.ReLU()),\n",
    "    nn.Sequential(UnitLength(), nn.Linear(500, 500), nn.ReLU()),\n",
    ").to(device)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training and test set\n",
    "def print_evaluation(epoch=None):\n",
    "    global model, x_tr, y_tr, x_te, y_te\n",
    "    error_rate = lambda x, y: 1.0 - torch.mean((x == y).float()).item()\n",
    "    prediction_error = lambda x, y: error_rate(predict(model, x, y), y)\n",
    "    train_error = prediction_error(x_tr, y_tr)\n",
    "    test_error = prediction_error(x_te, y_te)\n",
    "    epoch_str = 'init' if epoch is None else f\"{epoch:>4d}\"\n",
    "    print(f\"[{epoch_str}] Training: {train_error*100:>5.2f}%\\tTest: {test_error*100:>5.2f}%\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "torch.manual_seed(42)\n",
    "loss_fn = centroid_loss\n",
    "learning_rate = 0.05\n",
    "optimiser = Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 120+1\n",
    "batch_size = 4096\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print_evaluation()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Mini-batch training\n",
    "    for x, y in zip(split(x_tr, batch_size), split(y_tr, batch_size)):\n",
    "\n",
    "        # Train layers in turn, using backprop locally only\n",
    "        for layer in model:\n",
    "            h = layer(x)\n",
    "            loss = centroid_loss(h, y)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            x = h.detach() # no need to forward propagate x again, as direction doesn't change\n",
    "\n",
    "    # Evaluate the model on the training and test set\n",
    "    if (epoch + 1) % 5 == 1:\n",
    "        print_evaluation(epoch)\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}
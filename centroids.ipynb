{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bake Data"
      ],
      "metadata": {
        "id": "UiG20Kc3QjAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "save_path = './data/MNIST/baked/'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "file_path = lambda x: os.path.join(save_path, x)\n",
        "\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "train_set = MNIST('./data/', train=True, download=True, transform=transform)\n",
        "test_set = MNIST('./data/', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
        "\n",
        "train_x, train_y = next(iter(train_loader))\n",
        "test_x, test_y = next(iter(test_loader))\n",
        "\n",
        "torch.save(train_x, file_path('train_x.pt'))\n",
        "torch.save(train_y, file_path('train_y.pt'))\n",
        "torch.save(test_x, file_path('test_x.pt'))\n",
        "torch.save(test_y, file_path('test_y.pt'))"
      ],
      "metadata": {
        "id": "MWDiFmQ8QxB1",
        "outputId": "a868ce05-f5f5-4af0-b275-e5b84fe2f082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 387128928.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 102138021.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 136077984.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21550371.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "kmtk7h_nQq6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UnitLength(nn.Module):\n",
        "    \"\"\"Layer that normalises its inputs to a unit length vector\"\"\"\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x)\n",
        "    \n",
        "class LayerOutputs:\n",
        "    \"\"\"\n",
        "    Iterator that returns the output of each layer in a model, in turn. Model\n",
        "    must be an iterable of layers.\n",
        "    \n",
        "    Example:\n",
        "        >>> model = nn.Sequential(...) \n",
        "        >>> [h.mean() for h in LayerOutputs(model, x)]\n",
        "    \"\"\"\n",
        "    def __init__(self, model, x):\n",
        "        self.layers = iter(model)\n",
        "        self.x = x\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        layer = next(self.layers)\n",
        "        self.x = layer(self.x)\n",
        "        return self.x\n",
        "\n",
        "def visualise_sample(x, title='', sample_index=0):\n",
        "    img = x[sample_index].cpu().reshape(28, 28)\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.title(title)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yyh_Pu9HQTrF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CeIepmDGh2qJ",
        "outputId": "fd1e2274-2c8a-4e3a-8c55-fbcdcc2b8cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d70a588b9abc>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayerOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnitLength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import split\n",
        "from torch.optim import Adam\n",
        "from utils import LayerOutputs, UnitLength\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J91UxyCXh2qN"
      },
      "outputs": [],
      "source": [
        "def distance2_to_centroids(h, y_true, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared distance to the centroid of each class. \n",
        "\n",
        "    Returns a tensor of shape [n_examples, 10].\n",
        "    \"\"\"\n",
        "    safe_mean = lambda x, dim: x.sum(dim) / (x.shape[dim] + epsilon)\n",
        "    # TODO: what if class is missing? determine centroids only for classes that are present, and return torch.unique(y_true)\n",
        "    class_centroids = torch.stack([safe_mean(h[y_true == i],0) for i in range(10)], dim=1) # [n_in, 10]\n",
        "    x_to_centroids = h.unsqueeze(2) - class_centroids # [n_examples, n_in, 10]\n",
        "    return x_to_centroids.pow(2).mean(1) # [n_examples, 10]\n",
        "    \n",
        "@torch.no_grad()\n",
        "def predict(model, x, y_true):\n",
        "    \"\"\"Predict by finding the class with closest centroid to each example.\"\"\"\n",
        "    d = sum(distance2_to_centroids(h, y_true) for h in LayerOutputs(model, x))\n",
        "    return d.argmin(1) # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l7unkexh2qO"
      },
      "outputs": [],
      "source": [
        "def centroid_loss(h, y_true, alpha=10, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Loss function based on distance^2 to the true centroid vs a nearby centroid.\n",
        "    \n",
        "    Achieves an error rate of ~2.0%.\n",
        "    \"\"\"\n",
        "\n",
        "    # Distance from h to centroids of each class\n",
        "    d2 = distance2_to_centroids(h, y_true)\n",
        "\n",
        "    # Choose a nearby class, at random, using the inverse distance as a\n",
        "    # probability distribution\n",
        "    y_near = torch.multinomial((1 / (d2 + epsilon)), 1).squeeze(1)\n",
        "\n",
        "    # Smoothed version of triplet loss: max(0, d2_same - d2_near + margin)\n",
        "    d2_true = d2[range(d2.shape[0]), y_true] # ||anchor - positive||^2\n",
        "    d2_near = d2[range(d2.shape[0]), y_near] # ||anchor - negative||^2\n",
        "    return F.silu(alpha * (d2_true - d2_near)).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsancu9Hh2qO"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# The data is pre-processed, to speed up this script\n",
        "x_tr = torch.load('./data/MNIST/baked/train_x.pt', device)\n",
        "y_tr = torch.load('./data/MNIST/baked/train_y.pt', device)\n",
        "x_te = torch.load('./data/MNIST/baked/test_x.pt', device)\n",
        "y_te = torch.load('./data/MNIST/baked/test_y.pt', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWnUnFj4h2qP"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "# ----------------\n",
        "# Must be an iterable of layers. I find it works best if each layer starts with\n",
        "# a UnitLength() sub-layer.\n",
        "model = nn.Sequential(\n",
        "    nn.Sequential(UnitLength(), nn.Linear(784, 500), nn.ReLU()),\n",
        "    nn.Sequential(UnitLength(), nn.Linear(500, 500), nn.ReLU()),\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwM1zsSnh2qP"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the training and test set\n",
        "def print_evaluation(epoch=None):\n",
        "    global model, x_tr, y_tr, x_te, y_te\n",
        "    error_rate = lambda x, y: 1.0 - torch.mean((x == y).float()).item()\n",
        "    prediction_error = lambda x, y: error_rate(predict(model, x, y), y)\n",
        "    train_error = prediction_error(x_tr, y_tr)\n",
        "    test_error = prediction_error(x_te, y_te)\n",
        "    epoch_str = 'init' if epoch is None else f\"{epoch:>4d}\"\n",
        "    print(f\"[{epoch_str}] Training: {train_error*100:>5.2f}%\\tTest: {test_error*100:>5.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-apTR7Wh2qQ"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "torch.manual_seed(42)\n",
        "loss_fn = centroid_loss\n",
        "learning_rate = 0.05\n",
        "optimiser = Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 120+1\n",
        "batch_size = 4096\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j15TjyGqh2qR"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print_evaluation()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Mini-batch training\n",
        "    for x, y in zip(split(x_tr, batch_size), split(y_tr, batch_size)):\n",
        "\n",
        "        # Train layers in turn, using backprop locally only\n",
        "        for layer in model:\n",
        "            h = layer(x)\n",
        "            loss = centroid_loss(h, y)\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            x = h.detach() # no need to forward propagate x again, as direction doesn't change\n",
        "\n",
        "    # Evaluate the model on the training and test set\n",
        "    if (epoch + 1) % 5 == 1:\n",
        "        print_evaluation(epoch)\n"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  }
}
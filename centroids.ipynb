{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bake Data"
      ],
      "metadata": {
        "id": "UiG20Kc3QjAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "save_path = './data/MNIST/baked/'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "file_path = lambda x: os.path.join(save_path, x)\n",
        "\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "train_set = MNIST('./data/', train=True, download=True, transform=transform)\n",
        "test_set = MNIST('./data/', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
        "\n",
        "train_x, train_y = next(iter(train_loader))\n",
        "test_x, test_y = next(iter(test_loader))\n",
        "\n",
        "torch.save(train_x, file_path('train_x.pt'))\n",
        "torch.save(train_y, file_path('train_y.pt'))\n",
        "torch.save(test_x, file_path('test_x.pt'))\n",
        "torch.save(test_y, file_path('test_y.pt'))"
      ],
      "metadata": {
        "id": "MWDiFmQ8QxB1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "kmtk7h_nQq6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UnitLength(nn.Module):\n",
        "    \"\"\"Layer that normalises its inputs to a unit length vector\"\"\"\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x)\n",
        "    \n",
        "class LayerOutputs:\n",
        "    \"\"\"\n",
        "    Iterator that returns the output of each layer in a model, in turn. Model\n",
        "    must be an iterable of layers.\n",
        "    \n",
        "    Example:\n",
        "        >>> model = nn.Sequential(...) \n",
        "        >>> [h.mean() for h in LayerOutputs(model, x)]\n",
        "    \"\"\"\n",
        "    def __init__(self, model, x):\n",
        "        self.layers = iter(model)\n",
        "        self.x = x\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        layer = next(self.layers)\n",
        "        self.x = layer(self.x)\n",
        "        return self.x\n",
        "\n",
        "def visualise_sample(x, title='', sample_index=0):\n",
        "    img = x[sample_index].cpu().reshape(28, 28)\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.title(title)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yyh_Pu9HQTrF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Ukmed0O2SkpT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CeIepmDGh2qJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import split\n",
        "from torch.optim import Adam\n",
        "#from utils import LayerOutputs, UnitLength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "J91UxyCXh2qN"
      },
      "outputs": [],
      "source": [
        "def class_centroids(h, y_true):\n",
        "    \"\"\"\n",
        "    Calculates the the centroid of each class present in y.\n",
        "\n",
        "    Returns tensors of shape [n_in, n_classes_present].\n",
        "    \"\"\"\n",
        "    class_labels, y_idx = torch.unique(y, return_inverse=True) # [n_classes_present], [n_examples]\n",
        "    centroids = torch.stack([h[y == label].mean(0) for label in class_labels], dim=1) # [n_in, n_classes_present]\n",
        "    return centroids, y_idx\n",
        "\n",
        "def d(a, b, dim=1):\n",
        "    return (a - b).pow(2).mean(dim)\n",
        "\n",
        "def distance_to_centroids(h, y_true, d=d):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared distance to the centroid of every class. \n",
        "\n",
        "    Returns a tensor of shape [n_examples, n_in].\n",
        "    \"\"\"\n",
        "    centroids, _ = class_centroids(h, y_true)\n",
        "    return d(h.unsqueeze(2), centroids) # [n_examples, n_classes_present]\n",
        "    \n",
        "@torch.no_grad()\n",
        "def predict(model, x, y_true):\n",
        "    \"\"\"Predict by finding the class with closest centroid to each example.\"\"\"\n",
        "    << this won't work, as y_true will be different every time\n",
        "    d = sum(distance_to_centroids(h, y_true) for h in LayerOutputs(model, x))\n",
        "    return d.argmin(1) # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_l7unkexh2qO"
      },
      "outputs": [],
      "source": [
        "def centroid_loss(h, y_true, alpha=10, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Loss function based on distance^2 to the true centroid vs a nearby centroid.\n",
        "    \n",
        "    Achieves an error rate of ~2.0%.\n",
        "    \"\"\"\n",
        "\n",
        "    # Distance from h to centroids of each class\n",
        "    d2 = distance_to_centroids(h, y_true)\n",
        "\n",
        "    # Choose a nearby class, at random, using the inverse distance as a\n",
        "    # probability distribution\n",
        "    y_near = torch.multinomial((1 / (d2 + epsilon)), 1).squeeze(1)\n",
        "\n",
        "    # Smoothed version of triplet loss: max(0, d2_same - d2_near + margin)\n",
        "    d2_true = d2[range(d2.shape[0]), y_true] # ||anchor - positive||^2\n",
        "    d2_near = d2[range(d2.shape[0]), y_near] # ||anchor - negative||^2\n",
        "    return F.silu(alpha * (d2_true - d2_near)).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsancu9Hh2qO",
        "outputId": "0d5cb74b-70ef-4222-c8a2-91c0ca5e6605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# The data is pre-processed, to speed up this script\n",
        "x_tr = torch.load('./data/MNIST/baked/train_x.pt', device)\n",
        "y_tr = torch.load('./data/MNIST/baked/train_y.pt', device)\n",
        "x_te = torch.load('./data/MNIST/baked/test_x.pt', device)\n",
        "y_te = torch.load('./data/MNIST/baked/test_y.pt', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zWnUnFj4h2qP"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "# ----------------\n",
        "# Must be an iterable of layers. I find it works best if each layer starts with\n",
        "# a UnitLength() sub-layer.\n",
        "model = nn.Sequential(\n",
        "    nn.Sequential(UnitLength(), nn.Linear(784, 500), nn.ReLU()),\n",
        "    nn.Sequential(UnitLength(), nn.Linear(500, 500), nn.ReLU()),\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GwM1zsSnh2qP"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the training and test set\n",
        "def print_evaluation(epoch=None):\n",
        "    global model, x_tr, y_tr, x_te, y_te\n",
        "    error_rate = lambda x, y: 1.0 - torch.mean((x == y).float()).item()\n",
        "    prediction_error = lambda x, y: error_rate(predict(model, x, y), y)\n",
        "    train_error = prediction_error(x_tr, y_tr)\n",
        "    test_error = prediction_error(x_te, y_te)\n",
        "    epoch_str = 'init' if epoch is None else f\"{epoch:>4d}\"\n",
        "    print(f\"[{epoch_str}] Training: {train_error*100:>5.2f}%\\tTest: {test_error*100:>5.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "p-apTR7Wh2qQ"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "torch.manual_seed(42)\n",
        "loss_fn = centroid_loss\n",
        "learning_rate = 0.05\n",
        "optimiser = Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 120+1\n",
        "batch_size = 4096\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j15TjyGqh2qR",
        "outputId": "7239a5a0-448d-45d9-f264-cff2cc95ba78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[init] Training: 19.16%\tTest: 17.31%\n",
            "[   0] Training: 11.63%\tTest: 10.89%\n",
            "[   5] Training:  5.20%\tTest:  5.44%\n",
            "[  10] Training:  3.78%\tTest:  3.97%\n",
            "[  15] Training:  2.89%\tTest:  3.31%\n",
            "[  20] Training:  2.53%\tTest:  2.97%\n",
            "[  25] Training:  2.25%\tTest:  2.76%\n",
            "[  30] Training:  2.06%\tTest:  2.71%\n",
            "[  35] Training:  1.89%\tTest:  2.51%\n",
            "[  40] Training:  1.72%\tTest:  2.51%\n",
            "[  45] Training:  1.56%\tTest:  2.37%\n",
            "[  50] Training:  1.48%\tTest:  2.42%\n",
            "[  55] Training:  1.43%\tTest:  2.38%\n",
            "[  60] Training:  1.30%\tTest:  2.19%\n",
            "[  65] Training:  1.24%\tTest:  2.15%\n",
            "[  70] Training:  1.16%\tTest:  2.20%\n",
            "[  75] Training:  1.15%\tTest:  2.18%\n",
            "[  80] Training:  1.08%\tTest:  2.20%\n",
            "[  85] Training:  1.04%\tTest:  2.03%\n",
            "[  90] Training:  1.01%\tTest:  2.23%\n",
            "[  95] Training:  0.96%\tTest:  2.08%\n",
            "[ 100] Training:  0.89%\tTest:  2.02%\n",
            "[ 105] Training:  0.86%\tTest:  2.05%\n",
            "[ 110] Training:  0.82%\tTest:  2.08%\n",
            "[ 115] Training:  0.77%\tTest:  2.02%\n",
            "[ 120] Training:  0.79%\tTest:  2.09%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print_evaluation()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Mini-batch training\n",
        "    for x, y in zip(split(x_tr, batch_size), split(y_tr, batch_size)):\n",
        "\n",
        "        # Train layers in turn, using backprop locally only\n",
        "        for layer in model:\n",
        "            h = layer(x)\n",
        "            loss = centroid_loss(h, y)\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            x = h.detach() # no need to forward propagate x again, as direction doesn't change\n",
        "\n",
        "    # Evaluate the model on the training and test set\n",
        "    if (epoch + 1) % 5 == 1:\n",
        "        print_evaluation(epoch)\n"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  }
}
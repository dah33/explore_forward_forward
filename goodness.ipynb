{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import split\n",
    "from torch.optim import Adam\n",
    "from utils import LayerOutputs, UnitLength\n",
    "\n",
    "def superimpose_label(x, y):\n",
    "    x = x.clone()\n",
    "    x[:, :10] = 0\n",
    "    x[range(x.shape[0]), y] = x.max()\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodness(h): \n",
    "    \"\"\"Goodness is the *mean* squared activation of a layer.\"\"\"\n",
    "    return h.pow(2).mean(1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def goodness_per_class(model, x):\n",
    "    \"\"\"\n",
    "    Calculates the goodness for each class label.\n",
    "\n",
    "    This is the sum of goodness across all layers.\n",
    "     \n",
    "    Returns a tensor [n_examples, n_classes].\n",
    "    \"\"\"\n",
    "    g_per_label = []\n",
    "    for label in range(10):\n",
    "        x_candidate = superimpose_label(x, label)\n",
    "        g_candidate = sum(goodness(h) for h in LayerOutputs(model, x_candidate))\n",
    "        g_per_label.append(g_candidate.unsqueeze(1)) # type: ignore\n",
    "    return torch.cat(g_per_label, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, x):\n",
    "    \"\"\"Predict the class with highest goodness.\"\"\"\n",
    "    return goodness_per_class(model, x).argmax(1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_examples(model, x, y_true, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Make some positive and negative examples.\n",
    "    \n",
    "    The positive examples are superimposed with their true label. The negative\n",
    "    examples have a \"hard\" label with high goodness, excluding the true label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate goodness for each class label\n",
    "    g = goodness_per_class(model, x)\n",
    "\n",
    "    # Use the goodness as a probability distribution over all class labels.\n",
    "    # First, set true label probabilities to zero, then square root to make the\n",
    "    # distribution less peaked.\n",
    "    g[range(x.shape[0]), y_true] = 0\n",
    "    y_hard = torch.multinomial(torch.sqrt(g) + epsilon, 1).squeeze(1)\n",
    "\n",
    "    x_pos = superimpose_label(x, y_true)\n",
    "    x_neg = superimpose_label(x, y_hard)\n",
    "    return x_pos, x_neg\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinton_loss(h_pos, h_neg, theta=2.0):\n",
    "    \"\"\"\n",
    "    Calculate Hinton's Loss as per: https://arxiv.org/pdf/2212.13345.pdf\n",
    "\n",
    "    Converges very slowly. See SymBa paper for why:\n",
    "    https://arxiv.org/pdf/2303.08418.pdf\n",
    "\n",
    "    Achieves an error rate of ~2.7% after 600 epochs, with a learning rate of\n",
    "    0.1.\n",
    "\n",
    "    Paramaters:\n",
    "        theta (float): A margin used for both positive and negative examples.\n",
    "    \"\"\"\n",
    "    g_pos, g_neg = goodness(h_pos), goodness(h_neg)\n",
    "    return F.softplus(theta - g_pos).mean() + F.softplus(g_neg - theta).mean()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(h_pos, h_neg, margin=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the Triplet Loss.\n",
    "    \n",
    "    Adapted from the standard loss for Siamese Networks:\n",
    "    https://en.wikipedia.org/wiki/Triplet_loss\n",
    "\n",
    "    >>> Loss_i = max(||neg - anchor|| - ||pos - anchor|| + margin, 0)\n",
    "\n",
    "    We use goodness as the distance measure, where the \"anchor point\" is the\n",
    "    origin, i.e. zero goodness. We therefore want negative inputs to be near the\n",
    "    anchor, and positive inputs to be far from the anchor. Note, the anchor in\n",
    "    the standard formulation is for the positive input, but we use the anchor\n",
    "    for the negative input here, so the roles of negative and positive are\n",
    "    reversed. \n",
    "\n",
    "    Note, goodness is the sum of squared activations, so is the squared\n",
    "    Euclidean distance. The standard Euclidean distance works as well, but\n",
    "    converges somewhat slower.\n",
    "\n",
    "    Achieves an error rate of ~2.1%.\n",
    "    \"\"\"\n",
    "    g_pos, g_neg = goodness(h_pos), goodness(h_neg)\n",
    "    return F.relu(g_neg - g_pos + margin).mean()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_triplet_loss(h_pos, h_neg, alpha=5.0):\n",
    "    \"\"\"       \n",
    "    Calculate the Smoothed Triplet Loss.\n",
    "    \n",
    "    The Swish activation function (aka SiLU) acts like a smoothed ReLU. It bakes\n",
    "    in a margin of approx 2.6/alpha, and includes a smooth valley around the\n",
    "    margin point, which helps with convergence, and allows us to increase the\n",
    "    learning rate. \n",
    "    \n",
    "    The value of alpha determines the margin point. It also affects the\n",
    "    sharpness of the curvature around the margin, so an offset may be required\n",
    "    for more complex problems.\n",
    "\n",
    "    Achieves an error rate of ~1.7%.\n",
    "    \"\"\"\n",
    "    g_pos, g_neg = goodness(h_pos), goodness(h_neg)\n",
    "    return F.silu(alpha * (g_neg - g_pos)).mean()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# The data is pre-processed, to speed up this script\n",
    "x_tr = torch.load('./data/MNIST/baked/train_x.pt', device)\n",
    "y_tr = torch.load('./data/MNIST/baked/train_y.pt', device)\n",
    "x_te = torch.load('./data/MNIST/baked/test_x.pt', device)\n",
    "y_te = torch.load('./data/MNIST/baked/test_y.pt', device)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# ----------------\n",
    "# Must be an iterable of layers, each of which start with a\n",
    "# UnitLength() sub-layer, to \"conceal\" goodness.\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(UnitLength(), nn.Linear(784, 500), nn.ReLU()),\n",
    "    nn.Sequential(UnitLength(), nn.Linear(500, 500), nn.ReLU()),\n",
    ").to(device)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training and test set\n",
    "def print_evaluation(epoch=None):\n",
    "    global model, x_tr, y_tr, x_te, y_te\n",
    "    error_rate = lambda x, y: 1.0 - torch.mean((x == y).float()).item()\n",
    "    prediction_error = lambda x, y: error_rate(predict(model, x), y)\n",
    "    train_error = prediction_error(x_tr, y_tr)\n",
    "    test_error = prediction_error(x_te, y_te)\n",
    "    epoch_str = 'init' if epoch is None else f\"{epoch:>4d}\"\n",
    "    print(f\"[{epoch_str}] Training: {train_error*100:>5.2f}%\\tTest: {test_error*100:>5.2f}%\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "torch.manual_seed(42)\n",
    "loss_fn = smoothed_triplet_loss\n",
    "learning_rate = 0.35 if loss_fn is not hinton_loss else 0.1\n",
    "optimiser = Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 1 + (60 if loss_fn is not hinton_loss else 600)\n",
    "batch_size = 4096\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print_evaluation()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Mini-batch training\n",
    "    for x, y in zip(split(x_tr, batch_size), split(y_tr, batch_size)):\n",
    "\n",
    "        # Positive examples: the true label\n",
    "        # Negative examples: a \"hard\" label that is not the true label\n",
    "        # TODO: we could move the negative example generation inside the layer loop\n",
    "        x_pos, x_neg = make_examples(model, x, y)\n",
    "\n",
    "        # Train layers in turn, using backprop locally only\n",
    "        for layer in model:\n",
    "            h_pos, h_neg = layer(x_pos), layer(x_neg)\n",
    "            loss = loss_fn(h_pos, h_neg)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            x_pos, x_neg = h_pos.detach(), h_neg.detach()\n",
    "\n",
    "    # Evaluate the model on the training and test set\n",
    "    if (epoch + 1) % 5 == 1:\n",
    "        print_evaluation(epoch)\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}